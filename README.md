# Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs

The code in this repository can be used to run the experiments from the paper:

[**Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit Bias for Correlated Inputs**](https://arxiv.org/abs/2306.06479) <br>
*Dmitry Chistikov, Matthias Englert, Ranko Lazic*

## Requirements

To install required python packages run:

```
pip install -r requirements.txt
```

## Usage

Run with `python training_dynamics.py`.

The following command line options are supported:


```
    --d: The dimension.
    --n: The number of training points to use.
    --width: The width of the network.
    --learning-rate: The learning rate.
    --training-points-noise-scale: Training points are drawn at random (using a Gaussian) around
                                   a center point. The standard Guassians are multiplied by
                                   training_points_noise_scale / sqrt(d). Therefore, this option can
                                   be used to control the expected angles between points and the center.
                                   The default value is 1.0, unless the option uncentered-training-points
                                   is used in which case the default is sqrt(sqrt(2)-1). This ensures
                                   that the expected angle in either case is about 45 degrees.
    --uncentered-training-points: If this flag is added, uncentered instead of centered training points
                                  are generated.
    --two-teachers: If this flag is added, two teachers are used instead of one. The direction of the
                    second teacher is opposite to the first one. The norm of the first teacher neuron
                    is 1 and the norm of the second is 3.
    --init-scales: A single float or a list of floats indicating the initial scale for the weights.
                   If a list of numbers is given, a new experiment is run for each of them. However,
                   these experiments will share the same training points, and even the weights will be
                   the same apart from the different scaling.
    --cutoff: After how many iterations to stop the experiment. Defaults to 20 million.
    --loss-target: The experiment is stopped if the loss falls below this value. Defaults to 1e-9.
    --device: Can be used to specify the device to run the experiment on. Default is cpu,
              can be changed to cuda:0, cuda:1, etc. for the different GPUs if available.
    --output-dir: The directory where the experiment results are saved (as a csv file). If a list of
                  init scales is given, a new file is generated for each of the individual experiments.
    --save-to: Instead of running an experiment, generate inital weights, training points etc.
               and save these. Three files are generated. All start with what is given as a string
               to --save-to and then they have the respective ending -devicedata.pt, -data.pt,
               and -weights.pth.
    --load-from: Loads an experiment setup from files generated by the --save-to option. The files are
                 expected to have the same name as the --save-to option with the respective
                 endings -devicedata.pt, -data.pt, and -weights.pth. If this option is used,
                 some of the other options (such as d) are expecte to be consistent to what is stored
                 in the given files. However, other options (such as learning rate, cutoff, device, etc.)
                 can still be freely choosen. Most importantly, init scales can be freely choosen.
```

## Output

If an experiment is run, the output is written to a csv file. These files have a header in which each line start with a "#" character.
If these characters are removed, the resulting string is a valid json string containing a lot of information about the experiment that was run.
The rest of the file consists of a table stored in the csv format. Each row corresponds to one logging iteration and tracks a number of quantities in the different columns.

We suggest to use [Weights & Biases](https://wandb.ai) to visualize results as this does not require to work with the csv files.
Almost all statistics are also directly logged to Weights & Biases. To disable this, change "online" to "disabled" in the wandb.init call in main.py.
The project name can also be changed to something more meaningful in that function call.
